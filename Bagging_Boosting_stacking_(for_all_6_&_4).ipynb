{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/K9Uxxy9IoUz3Uwt6A/3V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorva14-unique/IML-PA/blob/main/Bagging_Boosting_stacking_(for_all_6_%26_4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging wih all features"
      ],
      "metadata": {
        "id": "Sd4fJnJqcig6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Loading and preprocessing data\n",
        "data = pd.read_csv('complete soil data.csv')\n",
        "\n",
        "# Cleaning data\n",
        "data = data[data['SOIL TYPE'].isin([1, 2, 3, 4, 5, 7, 8, 9, 11])]\n",
        "# data = data.dropna() # Remove or move this line\n",
        "\n",
        "# Outlier removal using IQR\n",
        "def remove_outliers(df, column):\n",
        "    # Ensure the column is numeric before calculating quantiles\n",
        "    # Non-numeric values will be coerced to NaN\n",
        "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "    # Drop rows where the value in this column became NaN after coercion\n",
        "    df = df.dropna(subset=[column])\n",
        "\n",
        "    # Now calculate quantiles on numeric data\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "    return df\n",
        "\n",
        "numerical_cols = ['PH', 'EC', 'OC', 'N', 'P2O5', 'K20', 'S', 'CU', 'FC', 'MN', 'ZN', 'BA', 'Temparature', 'Humidity', 'Rainfall']\n",
        "\n",
        "# Apply the outlier removal function, which now includes numeric conversion and dropna per column\n",
        "# Note: This processes each column sequentially and might drop rows multiple times.\n",
        "# An alternative is to convert all numerical columns first, then drop NaNs once, then remove outliers.\n",
        "# Keeping this loop structure but improving the function's robustness within the loop.\n",
        "for col in numerical_cols:\n",
        "    data = remove_outliers(data, col)\n",
        "\n",
        "# After processing numerical columns for outliers, drop any remaining rows with NaNs\n",
        "# that might exist in other columns (like categorical ones, though unlikely here)\n",
        "# or were not removed by the per-column dropna in remove_outliers (e.g., if a row had NaNs in multiple numerical columns)\n",
        "# It's safer to drop NaNs from the entire set of features and the target before further processing.\n",
        "features = ['MANDAL NAME', 'VILLAGE NAME', 'SOIL TYPE'] + numerical_cols # Re-define features to include categorical\n",
        "# Ensure 'CROP' column is included in the subset check if it might contain NaNs before encoding\n",
        "data = data.dropna(subset=features + ['CROP'])\n",
        "\n",
        "\n",
        "# Encoding categorical variables\n",
        "le_mandal = LabelEncoder()\n",
        "le_village = LabelEncoder()\n",
        "le_soil = LabelEncoder()\n",
        "le_crop = LabelEncoder()\n",
        "data['MANDAL NAME'] = le_mandal.fit_transform(data['MANDAL NAME'])\n",
        "data['VILLAGE NAME'] = le_village.fit_transform(data['VILLAGE NAME'])\n",
        "data['SOIL TYPE'] = le_soil.fit_transform(data['SOIL TYPE'])\n",
        "# Encode 'CROP' after dropping NaNs\n",
        "data['CROP'] = le_crop.fit_transform(data['CROP'])\n",
        "\n",
        "\n",
        "# Defining features and target\n",
        "X = data[features]\n",
        "y = data['CROP']\n",
        "\n",
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing numerical features\n",
        "scaler = StandardScaler()\n",
        "# Standardize only the numerical columns within X_train and X_test\n",
        "# Use pandas column indexing to select these columns and assign the scaled NumPy array back.\n",
        "\n",
        "# Note: scaler.fit_transform and scaler.transform return NumPy arrays.\n",
        "# When assigning a NumPy array to a slice of a DataFrame, pandas aligns by index.\n",
        "# This is safe here because the rows should correspond after the split.\n",
        "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
        "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
        "\n",
        "# --- Added code to handle minority classes before SMOTE ---\n",
        "# Check class distribution in y_train\n",
        "class_counts = y_train.value_counts()\n",
        "# Identify classes with fewer than SMOTE's default k_neighbors + 1 samples (5+1=6)\n",
        "# You might need to adjust this threshold depending on your data and SMOTE parameters\n",
        "min_samples_for_smote = 6 # Default k_neighbors (5) + 1\n",
        "\n",
        "# Get the labels of classes with insufficient samples\n",
        "classes_to_remove = class_counts[class_counts < min_samples_for_smote].index\n",
        "\n",
        "if not classes_to_remove.empty:\n",
        "    print(f\"Warning: Removing classes with fewer than {min_samples_for_smote} samples before SMOTE: {list(classes_to_remove)}\")\n",
        "    # Filter X_train and y_train to exclude samples from these classes\n",
        "    train_indices_to_keep = y_train[~y_train.isin(classes_to_remove)].index\n",
        "    X_train = X_train.loc[train_indices_to_keep]\n",
        "    y_train = y_train.loc[train_indices_to_keep]\n",
        "# --- End of added code ---\n",
        "\n",
        "# Handling class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "# Now apply SMOTE to the filtered training data\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "# Hyperparameter tuning for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Evaluating model\n",
        "y_pred = best_rf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "cv_scores = cross_val_score(best_rf, X_train, y_train, cv=5)\n",
        "\n",
        "print(f\"Bagging (Random Forest) with All Features\")\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Cross-Validation Accuracy: {np.mean(cv_scores * 100):.2f}% ± {np.std(cv_scores * 100):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw2T_H9luizW",
        "outputId": "291d900c-b10f-482d-ace5-ae5da1abe902"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Removing classes with fewer than 6 samples before SMOTE: [40, 23, 26, 9, 11, 31, 29, 44, 4, 37, 2, 25, 24, 27, 19, 32, 34, 15, 42, 16, 35, 0, 10, 8, 41, 21, 33, 22, 43, 14, 20, 3, 18]\n",
            "Bagging (Random Forest) with All Features\n",
            "Best Parameters: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Test Accuracy: 38.71%\n",
            "Cross-Validation Accuracy: 87.80% ± 3.66%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging with 6 features"
      ],
      "metadata": {
        "id": "lH7z6qPwcqcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Loading and preprocessing data\n",
        "data = pd.read_csv('complete soil data.csv')\n",
        "\n",
        "# Cleaning data\n",
        "data = data[data['SOIL TYPE'].isin([1, 2, 3, 4, 5, 7, 8, 9, 11])]\n",
        "\n",
        "# Outlier removal using IQR\n",
        "def remove_outliers(df, column):\n",
        "    # Assume the column is already numeric and contains no NaNs at this point\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    # Apply the filter directly to the DataFrame passed in\n",
        "    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "    return df\n",
        "\n",
        "numerical_cols = ['PH', 'EC', 'OC', 'N', 'P2O5', 'K20'] # Features used in this cell\n",
        "\n",
        "# --- Added steps for explicit numeric conversion and initial NaN drop ---\n",
        "# Explicitly convert the numerical columns to numeric, coercing errors to NaN\n",
        "for col in numerical_cols:\n",
        "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "\n",
        "# Drop rows where any of the relevant numerical columns (or the target 'CROP') have NaN\n",
        "# This step should happen *before* outlier removal\n",
        "data = data.dropna(subset=numerical_cols + ['CROP'])\n",
        "# --- End of added steps ---\n",
        "\n",
        "# Apply the outlier removal function *after* cleaning NaNs\n",
        "for col in numerical_cols:\n",
        "    # The remove_outliers function now assumes input data is clean (no NaNs in the column)\n",
        "    data = remove_outliers(data, col)\n",
        "\n",
        "\n",
        "# Encoding categorical variables\n",
        "le_crop = LabelEncoder()\n",
        "# The 'CROP' column is already cleaned for NaNs by the dropna call above\n",
        "data['CROP'] = le_crop.fit_transform(data['CROP'])\n",
        "\n",
        "# Defining features and target\n",
        "features = ['PH', 'EC', 'OC', 'N', 'P2O5', 'K20']\n",
        "X = data[features]\n",
        "y = data['CROP']\n",
        "\n",
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing numerical features\n",
        "scaler = StandardScaler()\n",
        "# X_train and X_test should now be clean and contain only numeric values\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Handling class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "# Check class distribution *before* SMOTE to identify minority classes with too few samples\n",
        "class_counts = y_train.value_counts()\n",
        "min_samples_for_smote = smote.k_neighbors + 1 # Default k_neighbors is 5, so minimum 6 samples\n",
        "\n",
        "# Identify classes with fewer than the minimum required samples\n",
        "classes_to_remove = class_counts[class_counts < min_samples_for_smote].index\n",
        "\n",
        "if not classes_to_remove.empty:\n",
        "    print(f\"Warning: Removing classes with fewer than {min_samples_for_smote} samples before SMOTE: {list(classes_to_remove)}\")\n",
        "    # Filter X_train and y_train to exclude samples from these classes\n",
        "    train_indices_to_keep = y_train[~y_train.isin(classes_to_remove)].index\n",
        "    X_train = pd.DataFrame(X_train, index=y_train.index, columns=features).loc[train_indices_to_keep].values # Reconstruct DataFrame to use .loc\n",
        "    y_train = y_train.loc[train_indices_to_keep]\n",
        "\n",
        "\n",
        "# Now apply SMOTE to the filtered training data\n",
        "# X_train is now a NumPy array after scaling. SMOTE expects array-like or sparse matrix.\n",
        "# y_train is a Series.\n",
        "# SMOTE's fit_resample returns NumPy arrays\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Hyperparameter tuning for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Evaluating model\n",
        "y_pred = best_rf.predict(X_test) # X_test is NumPy array\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "cv_scores = cross_val_score(best_rf, X_train, y_train, cv=5) # X_train, y_train are NumPy array and Series\n",
        "\n",
        "print(f\"Bagging (Random Forest) with 6 Features\")\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Cross-Validation Accuracy: {np.mean(cv_scores * 100):.2f}% ± {np.std(cv_scores * 100):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6iwSY94vEVv",
        "outputId": "d3eabdb3-016b-4df8-ef13-6535dbd8fa18"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Removing classes with fewer than 6 samples before SMOTE: [3, 39, 41, 16, 36, 21, 33, 35, 53, 47, 29, 5, 49, 28, 26, 25, 18, 44, 11, 20, 43, 23, 51, 22, 48, 15, 0, 54, 57, 27, 13, 42, 40, 10, 31, 14, 4]\n",
            "Bagging (Random Forest) with 6 Features\n",
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Test Accuracy: 15.15%\n",
            "Cross-Validation Accuracy: 84.46% ± 2.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging with 4 features"
      ],
      "metadata": {
        "id": "IJxV5xaZdLDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Loading and preprocessing data\n",
        "data = pd.read_csv('complete soil data.csv')\n",
        "\n",
        "# Cleaning data\n",
        "data = data[data['SOIL TYPE'].isin([1, 2, 3, 4, 5, 7, 8, 9, 11])]\n",
        "# data = data.dropna() # <--- Remove this line or move it\n",
        "\n",
        "# Outlier removal\n",
        "def remove_outliers(df, column):\n",
        "    # Assume the column is already numeric and contains no NaNs at this point\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    # Apply the filter directly to the DataFrame passed in\n",
        "    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "    return df\n",
        "\n",
        "numerical_cols = ['PH', 'EC', 'OC', 'N']\n",
        "\n",
        "# --- Added steps for explicit numeric conversion and initial NaN drop ---\n",
        "# Explicitly convert the numerical columns to numeric, coercing errors to NaN\n",
        "for col in numerical_cols:\n",
        "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "\n",
        "# Drop rows where any of the relevant numerical columns (or the target 'CROP') have NaN\n",
        "# This step should happen *before* outlier removal or before calling the outlier function\n",
        "data = data.dropna(subset=numerical_cols + ['CROP'])\n",
        "# --- End of added steps ---\n",
        "\n",
        "\n",
        "# Apply the outlier removal function *after* cleaning NaNs\n",
        "for col in numerical_cols:\n",
        "    # The remove_outliers function now assumes input data is clean (no NaNs in the column)\n",
        "    # The data variable is updated in each iteration.\n",
        "    data = remove_outliers(data, col)\n",
        "\n",
        "\n",
        "# Encoding categorical variables\n",
        "le_crop = LabelEncoder()\n",
        "# The 'CROP' column is already cleaned for NaNs by the dropna call above\n",
        "data['CROP'] = le_crop.fit_transform(data['CROP'])\n",
        "\n",
        "\n",
        "# Defining features and target\n",
        "features = ['PH', 'EC', 'OC', 'N']\n",
        "X = data[features]\n",
        "y = data['CROP']\n",
        "\n",
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing numerical features\n",
        "scaler = StandardScaler()\n",
        "# X_train and X_test should now be clean and contain only numeric values\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Handling class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "# Check class distribution *before* SMOTE to identify minority classes with too few samples\n",
        "class_counts = y_train.value_counts()\n",
        "min_samples_for_smote = smote.k_neighbors + 1 # Default k_neighbors is 5, so minimum 6 samples\n",
        "\n",
        "# Identify classes with fewer than the minimum required samples\n",
        "classes_to_remove = class_counts[class_counts < min_samples_for_smote].index\n",
        "\n",
        "if not classes_to_remove.empty:\n",
        "    print(f\"Warning: Removing classes with fewer than {min_samples_for_smote} samples before SMOTE: {list(classes_to_remove)}\")\n",
        "    # Filter X_train and y_train to exclude samples from these classes\n",
        "    train_indices_to_keep = y_train[~y_train.isin(classes_to_remove)].index\n",
        "    # When filtering X_train (which is a NumPy array after scaling), need to be careful with indexing\n",
        "    # A robust way is to convert X_train back to a DataFrame temporarily or use boolean indexing if indices align\n",
        "    # Since X_train is a NumPy array here, let's filter based on the original DataFrame index mapping\n",
        "    # This assumes the order of samples was preserved from the split to scaling.\n",
        "    # A more robust approach is to keep X_train as a DataFrame until after SMOTE.\n",
        "    # Let's stick to the current flow but use boolean indexing if possible or reconstruct DataFrame.\n",
        "    # Reconstructing DataFrame to use .loc indexing is safer:\n",
        "    X_train_df = pd.DataFrame(X_train, index=y_train.index, columns=features)\n",
        "    X_train_filtered_df = X_train_df.loc[train_indices_to_keep]\n",
        "    y_train_filtered = y_train.loc[train_indices_to_keep]\n",
        "    X_train = X_train_filtered_df.values # Convert back to NumPy array for SMOTE\n",
        "    y_train = y_train_filtered # Keep y_train as Series or convert to NumPy array if SMOTE needs it\n",
        "\n",
        "\n",
        "# Now apply SMOTE to the filtered training data\n",
        "# X_train is now a NumPy array after filtering. SMOTE expects array-like or sparse matrix.\n",
        "# y_train is a Series (or can be array).\n",
        "# SMOTE's fit_resample returns NumPy arrays\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "# Hyperparameter tuning for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Evaluating model\n",
        "y_pred = best_rf.predict(X_test) # X_test is NumPy array\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "cv_scores = cross_val_score(best_rf, X_train, y_train, cv=5) # X_train, y_train are NumPy array and Series\n",
        "\n",
        "print(f\"Bagging (Random Forest) with 4 Features\")\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Cross-Validation Accuracy: {np.mean(cv_scores * 100):.2f}% ± {np.std(cv_scores * 100):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYWZuA3Evzp4",
        "outputId": "c6e44c86-2d75-4050-dbd1-24e693d1c6fc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Removing classes with fewer than 6 samples before SMOTE: [36, 21, 16, 41, 49, 28, 25, 33, 29, 47, 35, 23, 53, 5, 10, 0, 54, 26, 13, 44, 27, 24, 22, 42, 9, 43, 57, 51, 14, 40, 30, 48, 1, 37, 3, 18, 31, 15, 17]\n",
            "Bagging (Random Forest) with 4 Features\n",
            "Best Parameters: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Test Accuracy: 19.27%\n",
            "Cross-Validation Accuracy: 78.30% ± 2.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "boosting with all features"
      ],
      "metadata": {
        "id": "l7VnvN0ad-Ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Loading and preprocessing data\n",
        "data = pd.read_csv('complete soil data.csv')\n",
        "\n",
        "# Cleaning data\n",
        "data = data[data['SOIL TYPE'].isin([1, 2, 3, 4, 5, 7, 8, 9, 11])]\n",
        "# data = data.dropna() # Moved this line\n",
        "\n",
        "# Outlier removal using IQR\n",
        "def remove_outliers(df, column):\n",
        "    # This function now assumes the input column is already numeric and doesn't contain NaNs\n",
        "    # The numeric conversion and NaN handling are done BEFORE calling this function.\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    # Apply the filter directly to the DataFrame passed in\n",
        "    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "    return df\n",
        "\n",
        "numerical_cols = ['PH', 'EC', 'OC', 'N', 'P2O5', 'K20', 'S', 'CU', 'FC', 'MN', 'ZN', 'BA', 'Temparature', 'Humidity', 'Rainfall']\n",
        "\n",
        "# --- Added explicit numeric conversion and initial NaN drop steps ---\n",
        "# Explicitly convert the numerical columns to numeric, coercing errors to NaN\n",
        "for col in numerical_cols:\n",
        "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "\n",
        "# Now drop rows with any missing values *after* coercing non-numeric to NaN.\n",
        "# Also ensure the target 'CROP' column is free of NaNs before encoding.\n",
        "data = data.dropna(subset=numerical_cols + ['CROP'])\n",
        "# --- End of added steps ---\n",
        "\n",
        "\n",
        "# Apply the outlier removal function *after* cleaning NaNs\n",
        "# The data variable is updated in each iteration.\n",
        "for col in numerical_cols:\n",
        "    # The remove_outliers function now assumes input data is clean (no NaNs in the column)\n",
        "    data = remove_outliers(data, col)\n",
        "\n",
        "\n",
        "# Encoding categorical variables\n",
        "le_mandal = LabelEncoder()\n",
        "le_village = LabelEncoder()\n",
        "le_soil = LabelEncoder()\n",
        "le_crop = LabelEncoder()\n",
        "# Ensure categorical columns are also free of NaNs before encoding if they were included in the initial dropna subset\n",
        "# In this case, MANDAL NAME, VILLAGE NAME, SOIL TYPE are likely clean by this point, but good practice to ensure.\n",
        "# data = data.dropna(subset=['MANDAL NAME', 'VILLAGE NAME', 'SOIL TYPE']) # Optional check if necessary\n",
        "data['MANDAL NAME'] = le_mandal.fit_transform(data['MANDAL NAME'])\n",
        "data['VILLAGE NAME'] = le_village.fit_transform(data['VILLAGE NAME'])\n",
        "data['SOIL TYPE'] = le_soil.fit_transform(data['SOIL TYPE'])\n",
        "# The 'CROP' column is already cleaned for NaNs by the dropna call above\n",
        "data['CROP'] = le_crop.fit_transform(data['CROP'])\n",
        "\n",
        "# Defining features and target\n",
        "# Ensure the features list only contains columns that are now numeric (encoded or converted)\n",
        "features = ['MANDAL NAME', 'VILLAGE NAME', 'SOIL TYPE'] + numerical_cols\n",
        "X = data[features]\n",
        "y = data['CROP']\n",
        "\n",
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing numerical features\n",
        "scaler = StandardScaler()\n",
        "# Apply scaler to the feature columns. Ensure all columns in X_train/X_test are numeric.\n",
        "# Categorical features are now integer encoded, which is fine for StandardScaler if treated as numerical (though scaling categorical might not always be ideal).\n",
        "# If you only want to scale the original numerical columns, you would need to select them here.\n",
        "# Based on the 'all features' description and previous cells, it seems the intention is to scale everything in X.\n",
        "# Let's assume scaling all features (encoded categorical + numerical) is the intent.\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Handling class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "# SMOTE works on NumPy arrays (X_train, y_train are now arrays)\n",
        "# Check class distribution *before* SMOTE to identify minority classes with too few samples\n",
        "# This check should use the y_train *before* SMOTE.\n",
        "# The previous dropna on 'CROP' should have removed rows with NaN targets.\n",
        "# However, SMOTE can still fail if some classes have < k_neighbors + 1 samples.\n",
        "# Need to re-evaluate y_train value counts after splitting but before SMOTE.\n",
        "\n",
        "# --- Added code to handle minority classes before SMOTE ---\n",
        "# Convert y_train back to Series temporarily to use value_counts efficiently\n",
        "y_train_series = pd.Series(y_train)\n",
        "class_counts = y_train_series.value_counts()\n",
        "min_samples_for_smote = smote.k_neighbors + 1 # Default k_neighbors is 5, so minimum 6 samples\n",
        "\n",
        "# Identify classes with fewer than the minimum required samples\n",
        "classes_to_remove = class_counts[class_counts < min_samples_for_smote].index\n",
        "\n",
        "if not classes_to_remove.empty:\n",
        "    print(f\"Warning: Removing classes with fewer than {min_samples_for_smote} samples before SMOTE: {list(classes_to_remove)}\")\n",
        "    # Filter X_train and y_train to exclude samples from these classes\n",
        "    # Need the original indices from the split to filter X_train (which is now a NumPy array)\n",
        "    # A safer approach is to keep X_train/y_train as DataFrames/Series until after potential filtering.\n",
        "    # Let's reconstruct DataFrame/Series from the scaled arrays and filter.\n",
        "    # This assumes the order is preserved from the split and scaling.\n",
        "    # Alternative: Filter before scaling if minority class removal is significant.\n",
        "    # Let's filter the scaled arrays by mapping back to original indices.\n",
        "    # This is tricky. A better way: Filter y_train (Series), get indices, apply to X_train (array).\n",
        "    # Assuming original indices are preserved in y_train Series after split:\n",
        "    train_indices_to_keep = y_train_series[~y_train_series.isin(classes_to_remove)].index\n",
        "    # Filter the NumPy array X_train using boolean indexing based on the indices from the Series\n",
        "    # Need to map Series index to array row index. This requires the original index.\n",
        "    # If X_train is a numpy array, its index is just 0...n-1. We cannot use .loc directly.\n",
        "    # Let's filter y_train (Series) first, then get the boolean mask, and apply it to X_train (ndarray).\n",
        "    boolean_mask = ~y_train_series.isin(classes_to_remove)\n",
        "    X_train = X_train[boolean_mask]\n",
        "    y_train = y_train[boolean_mask] # Keep y_train as NumPy array after filtering\n",
        "# --- End of added code ---\n",
        "\n",
        "# Now apply SMOTE to the filtered training data\n",
        "# X_train and y_train are now NumPy arrays after filtering and scaling.\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Hyperparameter tuning for Gradient Boosting\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(gb, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "# grid_search.fit expects array-like or sparse matrix, which X_train is after SMOTE\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_gb = grid_search.best_estimator_\n",
        "\n",
        "# Evaluating model\n",
        "# X_test is a NumPy array after scaling\n",
        "y_pred = best_gb.predict(X_test)\n",
        "# accuracy_score expects array-like or Series\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# cross_val_score expects array-like or sparse matrix for X, and array-like or Series for y\n",
        "cv_scores = cross_val_score(best_gb, X_train, y_train, cv=5)\n",
        "\n",
        "print(f\"Boosting (Gradient Boosting) with All Features\")\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Cross-Validation Accuracy: {np.mean(cv_scores * 100):.2f}% ± {np.std(cv_scores * 100):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBXUrMTMwYyB",
        "outputId": "f83f5dcd-2368-4f2d-e37c-87a4436fdd99"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Removing classes with fewer than 6 samples before SMOTE: [9, 11, 1, 24, 41, 34, 2, 27, 25, 4, 22, 28, 18, 38, 31, 39, 42, 17, 29, 0, 3, 10, 15, 30, 40, 20, 23, 14, 19, 32, 8]\n",
            "Boosting (Gradient Boosting) with All Features\n",
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
            "Test Accuracy: 32.20%\n",
            "Cross-Validation Accuracy: 87.51% ± 6.01%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting with 6 parameters"
      ],
      "metadata": {
        "id": "zxspuKZzfGuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Loading and preprocessing data\n",
        "data = pd.read_csv('complete soil data.csv')\n",
        "\n",
        "# Cleaning data\n",
        "data = data[data['SOIL TYPE'].isin([1, 2, 3, 4, 5, 7, 8, 9, 11])]\n",
        "# data = data.dropna() # Moved this line\n",
        "\n",
        "# Outlier removal\n",
        "def remove_outliers(df, column):\n",
        "    # This function now assumes the input column is already numeric and doesn't contain NaNs\n",
        "    # The numeric conversion and NaN handling are done BEFORE calling this function.\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    # Apply the filter directly to the DataFrame passed in\n",
        "    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "    return df\n",
        "\n",
        "numerical_cols = ['PH', 'EC', 'OC', 'N', 'P2O5', 'K20']\n",
        "\n",
        "# --- Added explicit numeric conversion and initial NaN drop steps ---\n",
        "# Explicitly convert the numerical columns to numeric, coercing errors to NaN\n",
        "for col in numerical_cols:\n",
        "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "\n",
        "# Now drop rows with any missing values *after* coercing non-numeric to NaN.\n",
        "# Also ensure the target 'CROP' column is free of NaNs before encoding.\n",
        "# Focus dropna on the columns relevant to this cell\n",
        "data = data.dropna(subset=numerical_cols + ['CROP'])\n",
        "# --- End of added steps ---\n",
        "\n",
        "\n",
        "# Apply the outlier removal function *after* cleaning NaNs\n",
        "# The data variable is updated in each iteration.\n",
        "for col in numerical_cols:\n",
        "    # The remove_outliers function now assumes input data is clean (no NaNs in the column)\n",
        "    data = remove_outliers(data, col)\n",
        "\n",
        "\n",
        "# Encoding categorical variables\n",
        "le_crop = LabelEncoder()\n",
        "# The 'CROP' column is already cleaned for NaNs by the dropna call above\n",
        "data['CROP'] = le_crop.fit_transform(data['CROP'])\n",
        "\n",
        "# Defining features and target\n",
        "features = ['PH', 'EC', 'OC', 'N', 'P2O5', 'K20']\n",
        "X = data[features]\n",
        "y = data['CROP']\n",
        "\n",
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing numerical features\n",
        "scaler = StandardScaler()\n",
        "# X_train and X_test should now be clean and contain only numeric values (from the numerical_cols list)\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Handling class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "# Check class distribution *before* SMOTE to identify minority classes with too few samples\n",
        "# SMOTE expects array-like or sparse matrix for X and array-like for y.\n",
        "# X_train is a NumPy array after scaling. y_train is a Series. SMOTE handles this mixed input.\n",
        "# However, the filtering logic needs to be careful with array indices vs Series indices.\n",
        "# A robust way is to keep X_train as a DataFrame until after filtering, then convert to NumPy.\n",
        "\n",
        "# --- Added code to handle minority classes before SMOTE ---\n",
        "# Convert y_train back to Series temporarily if it became an array, otherwise keep it as Series\n",
        "# In this cell, y_train starts as Series from the split, which is good.\n",
        "class_counts = y_train.value_counts()\n",
        "min_samples_for_smote = smote.k_neighbors + 1 # Default k_neighbors is 5, so minimum 6 samples\n",
        "\n",
        "# Identify classes with fewer than the minimum required samples\n",
        "classes_to_remove = class_counts[class_counts < min_samples_for_smote].index\n",
        "\n",
        "if not classes_to_remove.empty:\n",
        "    print(f\"Warning: Removing classes with fewer than {min_samples_for_smote} samples before SMOTE: {list(classes_to_remove)}\")\n",
        "    # Filter X_train (NumPy array) and y_train (Series)\n",
        "    # Filter y_train first using its index\n",
        "    train_indices_to_keep = y_train[~y_train.isin(classes_to_remove)].index\n",
        "\n",
        "    # To filter X_train (NumPy array) based on original Series indices,\n",
        "    # it's safer to reconstruct a DataFrame temporarily using the original index\n",
        "    # before filtering, then get the values back.\n",
        "    X_train_df = pd.DataFrame(X_train, index=y_train.index, columns=features)\n",
        "    X_train_filtered_df = X_train_df.loc[train_indices_to_keep]\n",
        "    y_train_filtered = y_train.loc[train_indices_to_keep]\n",
        "\n",
        "    X_train = X_train_filtered_df.values # Convert back to NumPy array for SMOTE\n",
        "    y_train = y_train_filtered # Keep y_train as Series (SMOTE accepts Series)\n",
        "# --- End of added code ---\n",
        "\n",
        "\n",
        "# Now apply SMOTE to the filtered training data\n",
        "# X_train is now a NumPy array. y_train is a Series.\n",
        "# SMOTE's fit_resample returns NumPy arrays\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "# Hyperparameter tuning for Gradient Boosting\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(gb, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "# grid_search.fit expects array-like or sparse matrix, which X_train is after SMOTE\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_gb = grid_search.best_estimator_\n",
        "\n",
        "# Evaluating model\n",
        "# X_test is a NumPy array after scaling\n",
        "y_pred = best_gb.predict(X_test)\n",
        "# accuracy_score expects array-like or Series\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# cross_val_score expects array-like or sparse matrix for X, and array-like or Series for y\n",
        "cv_scores = cross_val_score(best_gb, X_train, y_train, cv=5)\n",
        "\n",
        "print(f\"Boosting (Gradient Boosting) with 6 Features\")\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Cross-Validation Accuracy: {np.mean(cv_scores) * 100:.2f}% ± {np.std(cv_scores * 100):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45cFU1dC3KeL",
        "outputId": "530d9719-1f23-41b7-81cb-a0c7030859b8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Removing classes with fewer than 6 samples before SMOTE: [3, 39, 41, 16, 36, 21, 33, 35, 53, 47, 29, 5, 49, 28, 26, 25, 18, 44, 11, 20, 43, 23, 51, 22, 48, 15, 0, 54, 57, 27, 13, 42, 40, 10, 31, 14, 4]\n",
            "Boosting (Gradient Boosting) with 6 Features\n",
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
            "Test Accuracy: 18.18%\n",
            "Cross-Validation Accuracy: 80.49% ± 2.51%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting with 4 features"
      ],
      "metadata": {
        "id": "3d1N-aIOgCPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Loading and preprocessing data\n",
        "data = pd.read_csv('complete soil data.csv')\n",
        "\n",
        "# Cleaning data\n",
        "data = data[data['SOIL TYPE'].isin([1, 2, 3, 4, 5, 7, 8, 9, 11])]\n",
        "# data = data.dropna() # <--- Remove this line or move it. NaNs will be handled after coercion.\n",
        "\n",
        "# Outlier removal\n",
        "def remove_outliers(df, column):\n",
        "    # This function now assumes the input column is already numeric and doesn't contain NaNs\n",
        "    # The numeric conversion and NaN handling are done BEFORE calling this function.\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    # Apply the filter directly to the DataFrame passed in\n",
        "    # Use .copy() to avoid SettingWithCopyWarning when modifying subsets\n",
        "    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)].copy()\n",
        "    return df\n",
        "\n",
        "numerical_cols = ['PH', 'EC', 'OC', 'N']\n",
        "\n",
        "# --- Added explicit numeric conversion and initial NaN drop steps ---\n",
        "# Explicitly convert the numerical columns to numeric, coercing errors to NaN\n",
        "# This handles cases like \"0..07\" and turns them into NaN\n",
        "for col in numerical_cols:\n",
        "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "\n",
        "# Now drop rows with any missing values *after* coercing non-numeric to NaN.\n",
        "# Also ensure the target 'CROP' column is free of NaNs before encoding.\n",
        "# Focus dropna on the columns relevant to this cell (features + target)\n",
        "data = data.dropna(subset=numerical_cols + ['CROP'])\n",
        "# --- End of added steps ---\n",
        "\n",
        "\n",
        "# Apply the outlier removal function *after* cleaning NaNs\n",
        "# The data variable is updated in each iteration.\n",
        "for col in numerical_cols:\n",
        "    # The remove_outliers function now assumes input data is clean (no NaNs in the column being processed)\n",
        "    data = remove_outliers(data, col)\n",
        "\n",
        "\n",
        "# Encoding categorical variables\n",
        "le_crop = LabelEncoder()\n",
        "# The 'CROP' column is already cleaned for NaNs by the dropna call above\n",
        "data['CROP'] = le_crop.fit_transform(data['CROP'])\n",
        "\n",
        "\n",
        "# Defining features and target\n",
        "features = ['PH', 'EC', 'OC', 'N']\n",
        "X = data[features]\n",
        "y = data['CROP']\n",
        "\n",
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing numerical features\n",
        "scaler = StandardScaler()\n",
        "# X_train and X_test should now be clean and contain only numeric values\n",
        "# Ensure X_train columns are numeric before scaling (dropna above should handle this for the specified features)\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Handling class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "# Check class distribution *before* SMOTE to identify minority classes with too few samples\n",
        "# SMOTE expects array-like or sparse matrix for X and array-like for y.\n",
        "# X_train is a NumPy array after scaling. y_train is a Series. SMOTE handles this mixed input.\n",
        "# However, the filtering logic needs to be careful with array indices vs Series indices.\n",
        "# A robust way is to keep X_train as a DataFrame until after filtering, then convert to NumPy.\n",
        "\n",
        "# --- Added code to handle minority classes before SMOTE ---\n",
        "# Convert y_train back to Series temporarily if it became an array, otherwise keep it as Series\n",
        "# In this cell, y_train starts as Series from the split, which is good.\n",
        "class_counts = y_train.value_counts()\n",
        "min_samples_for_smote = smote.k_neighbors + 1 # Default k_neighbors is 5, so minimum 6 samples\n",
        "\n",
        "# Identify classes with fewer than the minimum required samples\n",
        "classes_to_remove = class_counts[class_counts < min_samples_for_smote].index\n",
        "\n",
        "if not classes_to_remove.empty:\n",
        "    print(f\"Warning: Removing classes with fewer than {min_samples_for_smote} samples before SMOTE: {list(classes_to_remove)}\")\n",
        "    # Filter X_train (NumPy array) and y_train (Series)\n",
        "    # Filter y_train first using its index\n",
        "    train_indices_to_keep = y_train[~y_train.isin(classes_to_remove)].index\n",
        "\n",
        "    # To filter X_train (NumPy array) based on original Series indices,\n",
        "    # it's safer to reconstruct a DataFrame temporarily using the original index\n",
        "    # before filtering, then get the values back.\n",
        "    # This assumes the order is preserved from the split to scaling/filtering.\n",
        "    # It's best practice to filter the original DataFrames/Series first.\n",
        "    # Let's try a direct boolean indexing on the NumPy array using the mask derived from y_train's indices.\n",
        "    # This is safer if the indices of y_train Series match the row order of X_train NumPy array.\n",
        "    boolean_mask = y_train.index.isin(train_indices_to_keep) # Create boolean mask from indices\n",
        "    X_train = X_train[boolean_mask] # Apply boolean mask to NumPy array\n",
        "    y_train = y_train.loc[train_indices_to_keep] # Filter y_train Series using .loc\n",
        "# --- End of added code ---\n",
        "\n",
        "\n",
        "# Now apply SMOTE to the filtered training data\n",
        "# X_train is now a NumPy array. y_train is a Series.\n",
        "# SMOTE's fit_resample returns NumPy arrays\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "# Hyperparameter tuning for Gradient Boosting\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(gb, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "# grid_search.fit expects array-like or sparse matrix, which X_train is after SMOTE\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_gb = grid_search.best_estimator_\n",
        "\n",
        "# Evaluating model\n",
        "# X_test is a NumPy array after scaling\n",
        "y_pred = best_gb.predict(X_test)\n",
        "# accuracy_score expects array-like or Series\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# cross_val_score expects array-like or sparse matrix for X, and array-like or Series for y\n",
        "cv_scores = cross_val_score(best_gb, X_train, y_train, cv=5)\n",
        "\n",
        "print(f\"Boosting (Gradient Boosting) with 4 Features\")\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Cross-Validation Accuracy: {np.mean(cv_scores * 100):.2f}% ± {np.std(cv_scores) * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaNTCavJ8J1u",
        "outputId": "c80cf232-1bb0-4e26-a8d8-da5c68c8e716"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Removing classes with fewer than 6 samples before SMOTE: [36, 21, 16, 41, 49, 28, 25, 33, 29, 47, 35, 23, 53, 5, 10, 0, 54, 26, 13, 44, 27, 24, 22, 42, 9, 43, 57, 51, 14, 40, 30, 48, 1, 37, 3, 18, 31, 15, 17]\n",
            "Boosting (Gradient Boosting) with 4 Features\n",
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n",
            "Test Accuracy: 20.18%\n",
            "Cross-Validation Accuracy: 71.05% ± 3.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacking with all features"
      ],
      "metadata": {
        "id": "rThYJnVsg_AY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Loading and preprocessing data\n",
        "data = pd.read_csv('complete soil data.csv')\n",
        "\n",
        "# Cleaning data\n",
        "data = data[data['SOIL TYPE'].isin([1, 2, 3, 4, 5, 7, 8, 9, 11])]\n",
        "# data = data.dropna() # <-- REMOVED this line or MOVED it. NaNs will be handled AFTER coercion.\n",
        "\n",
        "# Outlier removal\n",
        "def remove_outliers(df, column):\n",
        "    # This function now assumes the input column is already numeric and doesn't contain NaNs\n",
        "    # The numeric conversion and NaN handling are done BEFORE calling this function.\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    # Apply the filter directly to the DataFrame passed in\n",
        "    # Use .copy() to avoid SettingWithCopyWarning when modifying subsets\n",
        "    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)].copy()\n",
        "    return df\n",
        "\n",
        "numerical_cols = ['PH', 'EC', 'OC', 'N', 'P2O5', 'K20', 'S', 'CU', 'FC', 'MN', 'ZN', 'BA', 'Temparature', 'Humidity', 'Rainfall']\n",
        "\n",
        "# --- ADDED: Explicitly convert numerical columns to numeric BEFORE outlier removal and dropna ---\n",
        "# Explicitly convert the numerical columns to numeric, coercing errors to NaN\n",
        "# This handles cases like \"0..07\" and turns them into NaN\n",
        "for col in numerical_cols:\n",
        "    # Added try-except for more robust conversion, although errors='coerce' should handle most\n",
        "    try:\n",
        "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "    except ValueError as e:\n",
        "        print(f\"Error converting column {col} to numeric: {e}\")\n",
        "        # If a specific value is known to cause issues, you might target it here\n",
        "        # For instance: data[col] = data[col].replace('0..07', np.nan)\n",
        "        data[col] = pd.to_numeric(data[col], errors='coerce') # Re-attempt with coerce\n",
        "# --- END ADDED ---\n",
        "\n",
        "# --- ADDED: Drop rows with any missing values AFTER coercing non-numeric to NaN ---\n",
        "# Now drop rows with any missing values (including those created by coercion).\n",
        "# Also ensure the target 'CROP' column is free of NaNs before encoding.\n",
        "# Focus dropna on the columns relevant to this cell (features + target)\n",
        "features_in_this_cell = ['MANDAL NAME', 'VILLAGE NAME', 'SOIL TYPE'] + numerical_cols\n",
        "data = data.dropna(subset=features_in_this_cell + ['CROP'])\n",
        "# --- END ADDED ---\n",
        "\n",
        "\n",
        "# Apply the outlier removal function *after* cleaning NaNs\n",
        "# The data variable is updated in each iteration.\n",
        "# The remove_outliers function now assumes input data is clean (no NaNs in the column being processed)\n",
        "# since NaNs were dropped in the step above.\n",
        "for col in numerical_cols:\n",
        "    data = remove_outliers(data, col)\n",
        "\n",
        "\n",
        "# Encoding categorical variables\n",
        "le_mandal = LabelEncoder()\n",
        "le_village = LabelEncoder()\n",
        "le_soil = LabelEncoder()\n",
        "le_crop = LabelEncoder()\n",
        "# Categorical columns should be clean of NaNs by the dropna step above, assuming they were included.\n",
        "# Re-check the subset in dropna if categorical columns might have NaNs and are needed for features.\n",
        "data['MANDAL NAME'] = le_mandal.fit_transform(data['MANDAL NAME'])\n",
        "data['VILLAGE NAME'] = le_village.fit_transform(data['VILLAGE NAME'])\n",
        "data['SOIL TYPE'] = le_soil.fit_transform(data['SOIL TYPE'])\n",
        "# The 'CROP' column is already cleaned for NaNs by the dropna call above\n",
        "data['CROP'] = le_crop.fit_transform(data['CROP'])\n",
        "\n",
        "# Defining features and target\n",
        "# Ensure the features list only contains columns that are now numeric (encoded or converted)\n",
        "features = ['MANDAL NAME', 'VILLAGE NAME', 'SOIL TYPE'] + numerical_cols\n",
        "X = data[features]\n",
        "y = data['CROP']\n",
        "\n",
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing numerical features\n",
        "scaler = StandardScaler()\n",
        "# Apply scaler to the feature columns. Ensure all columns in X_train/X_test are numeric.\n",
        "# Categorical features are now integer encoded, which is fine for StandardScaler if treated as numerical.\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Handling class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "# SMOTE expects array-like or sparse matrix for X and array-like for y.\n",
        "# X_train is a NumPy array after scaling. y_train is a Series. SMOTE handles this mixed input.\n",
        "# Check class distribution *before* SMOTE to identify minority classes with too few samples\n",
        "# Need to re-evaluate y_train value counts after splitting but before SMOTE.\n",
        "\n",
        "# --- ADDED code to handle minority classes before SMOTE ---\n",
        "# Convert y_train to Series if it's not already, to use value_counts efficiently\n",
        "# It should be a Series from train_test_split here.\n",
        "class_counts = y_train.value_counts()\n",
        "min_samples_for_smote = smote.k_neighbors + 1 # Default k_neighbors is 5, so minimum 6 samples\n",
        "\n",
        "# Identify classes with fewer than the minimum required samples\n",
        "classes_to_remove = class_counts[class_counts < min_samples_for_smote].index\n",
        "\n",
        "if not classes_to_remove.empty:\n",
        "    print(f\"Warning: Removing classes with fewer than {min_samples_for_smote} samples before SMOTE: {list(classes_to_remove)}\")\n",
        "    # Filter X_train (NumPy array) and y_train (Series)\n",
        "    # Filter y_train first using its index\n",
        "    train_indices_to_keep = y_train[~y_train.isin(classes_to_remove)].index\n",
        "\n",
        "    # To filter X_train (NumPy array) based on original Series indices,\n",
        "    # use boolean indexing based on the indices from the Series\n",
        "    # This is safer if the indices of y_train Series match the row order of X_train NumPy array.\n",
        "    boolean_mask = y_train.index.isin(train_indices_to_keep) # Create boolean mask from indices\n",
        "    X_train = X_train[boolean_mask] # Apply boolean mask to NumPy array\n",
        "    y_train = y_train.loc[train_indices_to_keep] # Filter y_train Series using .loc\n",
        "# --- END ADDED code ---\n",
        "\n",
        "\n",
        "# Now apply SMOTE to the filtered training data\n",
        "# X_train is now a NumPy array. y_train is a Series.\n",
        "# SMOTE's fit_resample returns NumPy arrays\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "# Defining base models for stacking\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42)),\n",
        "    ('gb', GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42)),\n",
        "    ('xgb', XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42, use_label_encoder=False, eval_metric='mlogloss')) # Added use_label_encoder=False for modern XGBoost\n",
        "]\n",
        "\n",
        "# Training Stacking Classifier with Random Forest meta-classifier\n",
        "stacking = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(n_estimators=100, random_state=42), cv=5)\n",
        "# grid_search.fit expects array-like or sparse matrix, which X_train is after SMOTE\n",
        "stacking.fit(X_train, y_train) # Fit the stacking classifier\n",
        "\n",
        "# Evaluating model\n",
        "# X_test is a NumPy array after scaling\n",
        "y_pred = stacking.predict(X_test)\n",
        "# accuracy_score expects array-like or Series\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# cross_val_score expects array-like or sparse matrix for X, and array-like or Series for y\n",
        "cv_scores = cross_val_score(stacking, X_train, y_train, cv=5) # Evaluate the stacking classifier\n",
        "\n",
        "print(f\"Stacking Classifier with All Features\")\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Cross-Validation Accuracy: {np.mean(cv_scores * 100):.2f}% ± {np.std(cv_scores * 100):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq4USE7V_00U",
        "outputId": "c2b7789a-eee5-411b-f08f-c47182e51e23"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Removing classes with fewer than 6 samples before SMOTE: [9, 11, 1, 24, 41, 34, 2, 27, 25, 4, 22, 28, 18, 38, 31, 39, 42, 17, 29, 0, 3, 10, 15, 30, 40, 20, 23, 14, 19, 32, 8]\n",
            "Stacking Classifier with All Features\n",
            "Test Accuracy: 35.59%\n",
            "Cross-Validation Accuracy: 90.48% ± 6.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacking with 6 features"
      ],
      "metadata": {
        "id": "Q5vOhTbyi1uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Loading and preprocessing data\n",
        "data = pd.read_csv('complete soil data.csv')\n",
        "\n",
        "# Cleaning data\n",
        "data = data[data['SOIL TYPE'].isin([1, 2, 3, 4, 5, 7, 8, 9, 11])]\n",
        "\n",
        "# Define the numerical features for this specific cell\n",
        "numerical_cols = ['PH', 'EC', 'OC', 'N', 'P2O5', 'K20']\n",
        "\n",
        "# --- ADDED: Explicitly convert numerical columns to numeric, coercing errors ---\n",
        "# This handles cases like \"0..07\" and turns them into NaN\n",
        "for col in numerical_cols:\n",
        "    # Use errors='coerce' to turn problematic values into NaN\n",
        "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "# --- END ADDED ---\n",
        "\n",
        "# --- MODIFIED: Drop rows with any missing values AFTER coercing non-numeric to NaN ---\n",
        "# Now drop rows with any missing values (including those created by coercion)\n",
        "# Focus dropna on the columns relevant to this cell (numerical features + target)\n",
        "data = data.dropna(subset=numerical_cols + ['CROP'])\n",
        "# --- END MODIFIED ---\n",
        "\n",
        "# Outlier removal\n",
        "def remove_outliers(df, column):\n",
        "    # This function now assumes the input column is already numeric and doesn't contain NaNs,\n",
        "    # because the explicit conversion and dropna steps were done BEFORE calling this function.\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    # Apply the filter directly to the DataFrame passed in\n",
        "    # Use .copy() to avoid SettingWithCopyWarning when modifying subsets\n",
        "    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)].copy()\n",
        "    return df\n",
        "\n",
        "# Apply the outlier removal function *after* cleaning NaNs\n",
        "for col in numerical_cols:\n",
        "    # The data variable is updated in each iteration.\n",
        "    # The remove_outliers function now assumes input data is clean (no NaNs in the column being processed)\n",
        "    # since NaNs were dropped in the step above.\n",
        "    data = remove_outliers(data, col)\n",
        "\n",
        "\n",
        "# Encoding categorical variables\n",
        "le_crop = LabelEncoder()\n",
        "# The 'CROP' column is already cleaned for NaNs by the dropna call above\n",
        "data['CROP'] = le_crop.fit_transform(data['CROP'])\n",
        "\n",
        "# Defining features and target\n",
        "features = ['PH', 'EC', 'OC', 'N', 'P2O5', 'K20'] # Features used in this cell\n",
        "X = data[features]\n",
        "y = data['CROP']\n",
        "\n",
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing numerical features\n",
        "scaler = StandardScaler()\n",
        "# X_train and X_test should now be clean and contain only numeric values (from the numerical_cols list)\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Handling class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "# SMOTE expects array-like or sparse matrix for X and array-like for y.\n",
        "# X_train is a NumPy array after scaling. y_train is a Series. SMOTE handles this mixed input.\n",
        "# Check class distribution *before* SMOTE to identify minority classes with too few samples\n",
        "# Need to re-evaluate y_train value counts after splitting but before SMOTE.\n",
        "\n",
        "# --- ADDED code to handle minority classes before SMOTE ---\n",
        "# Convert y_train to Series if it's not already, to use value_counts efficiently\n",
        "# It should be a Series from train_test_split here.\n",
        "class_counts = y_train.value_counts()\n",
        "min_samples_for_smote = smote.k_neighbors + 1 # Default k_neighbors is 5, so minimum 6 samples\n",
        "\n",
        "# Identify classes with fewer than the minimum required samples\n",
        "classes_to_remove = class_counts[class_counts < min_samples_for_smote].index\n",
        "\n",
        "if not classes_to_remove.empty:\n",
        "    print(f\"Warning: Removing classes with fewer than {min_samples_for_smote} samples before SMOTE: {list(classes_to_remove)}\")\n",
        "    # Filter X_train (NumPy array) and y_train (Series)\n",
        "    # Filter y_train first using its index\n",
        "    train_indices_to_keep = y_train[~y_train.isin(classes_to_remove)].index\n",
        "\n",
        "    # To filter X_train (NumPy array) based on original Series indices,\n",
        "    # use boolean indexing based on the indices from the Series\n",
        "    # This is safer if the indices of y_train Series match the row order of X_train NumPy array.\n",
        "    # Create boolean mask based on original Series indices\n",
        "    boolean_mask = y_train.index.isin(train_indices_to_keep)\n",
        "    X_train = X_train[boolean_mask] # Apply boolean mask to NumPy array\n",
        "    y_train = y_train.loc[train_indices_to_keep] # Filter y_train Series using .loc\n",
        "# --- END ADDED code ---\n",
        "\n",
        "\n",
        "# Now apply SMOTE to the filtered training data\n",
        "# X_train is now a NumPy array. y_train is a Series.\n",
        "# SMOTE's fit_resample returns NumPy arrays\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "# Defining base models for stacking\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42)),\n",
        "    ('gb', GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42)),\n",
        "    # Added use_label_encoder=False for modern XGBoost to avoid deprecation warnings\n",
        "    ('xgb', XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42, use_label_encoder=False, eval_metric='mlogloss'))\n",
        "]\n",
        "\n",
        "# Training Stacking Classifier with Random Forest meta-classifier\n",
        "stacking = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(n_estimators=100, random_state=42), cv=5)\n",
        "# grid_search.fit expects array-like or sparse matrix, which X_train is after SMOTE\n",
        "stacking.fit(X_train, y_train) # Fit the stacking classifier\n",
        "\n",
        "# Evaluating model\n",
        "# X_test is a NumPy array after scaling\n",
        "y_pred = stacking.predict(X_test)\n",
        "# accuracy_score expects array-like or Series\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# cross_val_score expects array-like or sparse matrix for X, and array-like or Series for y\n",
        "cv_scores = cross_val_score(stacking, X_train, y_train, cv=5) # Evaluate the stacking classifier\n",
        "\n",
        "print(f\"Stacking Classifier with 6 Features\")\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Cross-Validation Accuracy: {np.mean(cv_scores * 100):.2f}% ± {np.std(cv_scores * 100):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARPuPfo8BZob",
        "outputId": "0a0640b7-e458-46a9-ce62-cfbbcfa1fc3e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Removing classes with fewer than 6 samples before SMOTE: [3, 39, 41, 16, 36, 21, 33, 35, 53, 47, 29, 5, 49, 28, 26, 25, 18, 44, 11, 20, 43, 23, 51, 22, 48, 15, 0, 54, 57, 27, 13, 42, 40, 10, 31, 14, 4]\n",
            "Stacking Classifier with 6 Features\n",
            "Test Accuracy: 19.19%\n",
            "Cross-Validation Accuracy: 85.13% ± 3.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacking with 4 features"
      ],
      "metadata": {
        "id": "C9Od7wYZjJrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Loading and preprocessing data\n",
        "data = pd.read_csv('complete soil data.csv')\n",
        "\n",
        "# Cleaning data\n",
        "data = data[data['SOIL TYPE'].isin([1, 2, 3, 4, 5, 7, 8, 9, 11])]\n",
        "\n",
        "# Define the numerical features for this specific cell\n",
        "numerical_cols = ['PH', 'EC', 'OC', 'N']\n",
        "\n",
        "# --- ADDED: Explicitly convert numerical columns to numeric, coercing errors ---\n",
        "# This handles cases like \"0..07\" and turns them into NaN\n",
        "for col in numerical_cols:\n",
        "    # Use errors='coerce' to turn problematic values into NaN\n",
        "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "# --- END ADDED ---\n",
        "\n",
        "# --- MODIFIED: Drop rows with any missing values AFTER coercing non-numeric to NaN ---\n",
        "# Now drop rows with any missing values (including those created by coercion)\n",
        "# Focus dropna on the columns relevant to this cell (numerical features + target)\n",
        "data = data.dropna(subset=numerical_cols + ['CROP'])\n",
        "# --- END MODIFIED ---\n",
        "\n",
        "# Outlier removal\n",
        "def remove_outliers(df, column):\n",
        "    # This function now assumes the input column is already numeric and doesn't contain NaNs,\n",
        "    # because the explicit conversion and dropna steps were done BEFORE calling this function.\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    # Apply the filter directly to the DataFrame passed in\n",
        "    # Use .copy() to avoid SettingWithCopyWarning when modifying subsets\n",
        "    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)].copy()\n",
        "    return df\n",
        "\n",
        "# Apply the outlier removal function *after* cleaning NaNs\n",
        "for col in numerical_cols:\n",
        "    # The data variable is updated in each iteration.\n",
        "    # The remove_outliers function now assumes input data is clean (no NaNs in the column being processed)\n",
        "    # since NaNs were dropped in the step above.\n",
        "    data = remove_outliers(data, col)\n",
        "\n",
        "\n",
        "# Encoding categorical variables\n",
        "le_crop = LabelEncoder()\n",
        "# The 'CROP' column is already cleaned for NaNs by the dropna call above\n",
        "data['CROP'] = le_crop.fit_transform(data['CROP'])\n",
        "\n",
        "# Defining features and target\n",
        "features = ['PH', 'EC', 'OC', 'N'] # Features used in this cell\n",
        "X = data[features]\n",
        "y = data['CROP']\n",
        "\n",
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing numerical features\n",
        "scaler = StandardScaler()\n",
        "# X_train and X_test should now be clean and contain only numeric values (from the numerical_cols list)\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Handling class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "# SMOTE expects array-like or sparse matrix for X and array-like for y.\n",
        "# X_train is a NumPy array after scaling. y_train is a Series. SMOTE handles this mixed input.\n",
        "# Check class distribution *before* SMOTE to identify minority classes with too few samples\n",
        "# Need to re-evaluate y_train value counts after splitting but before SMOTE.\n",
        "\n",
        "# --- ADDED code to handle minority classes before SMOTE ---\n",
        "# Convert y_train to Series if it's not already, to use value_counts efficiently\n",
        "# It should be a Series from train_test_split here.\n",
        "class_counts = y_train.value_counts()\n",
        "min_samples_for_smote = smote.k_neighbors + 1 # Default k_neighbors is 5, so minimum 6 samples\n",
        "\n",
        "# Identify classes with fewer than the minimum required samples\n",
        "classes_to_remove = class_counts[class_counts < min_samples_for_smote].index\n",
        "\n",
        "if not classes_to_remove.empty:\n",
        "    print(f\"Warning: Removing classes with fewer than {min_samples_for_smote} samples before SMOTE: {list(classes_to_remove)}\")\n",
        "    # Filter X_train (NumPy array) and y_train (Series)\n",
        "    # Filter y_train first using its index\n",
        "    train_indices_to_keep = y_train[~y_train.isin(classes_to_remove)].index\n",
        "\n",
        "    # To filter X_train (NumPy array) based on original Series indices,\n",
        "    # use boolean indexing based on the indices from the Series\n",
        "    # This is safer if the indices of y_train Series match the row order of X_train NumPy array.\n",
        "    # Create boolean mask based on original Series indices\n",
        "    boolean_mask = y_train.index.isin(train_indices_to_keep)\n",
        "    X_train = X_train[boolean_mask] # Apply boolean mask to NumPy array\n",
        "    y_train = y_train.loc[train_indices_to_keep] # Filter y_train Series using .loc\n",
        "# --- END ADDED code ---\n",
        "\n",
        "\n",
        "# Now apply SMOTE to the filtered training data\n",
        "# X_train is now a NumPy array. y_train is a Series.\n",
        "# SMOTE's fit_resample returns NumPy arrays\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "# Defining base models for stacking\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42)),\n",
        "    ('gb', GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42)),\n",
        "    # Added use_label_encoder=False for modern XGBoost to avoid deprecation warnings\n",
        "    ('xgb', XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42, use_label_encoder=False, eval_metric='mlogloss'))\n",
        "]\n",
        "\n",
        "# Training Stacking Classifier with Random Forest meta-classifier\n",
        "stacking = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(n_estimators=100, random_state=42), cv=5)\n",
        "# grid_search.fit expects array-like or sparse matrix, which X_train is after SMOTE\n",
        "stacking.fit(X_train, y_train) # Fit the stacking classifier\n",
        "\n",
        "# Evaluating model\n",
        "# X_test is a NumPy array after scaling\n",
        "y_pred = stacking.predict(X_test)\n",
        "# accuracy_score expects array-like or Series\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# cross_val_score expects array-like or sparse matrix for X, and array-like or Series for y\n",
        "cv_scores = cross_val_score(stacking, X_train, y_train, cv=5) # Evaluate the stacking classifier\n",
        "\n",
        "print(f\"Stacking Classifier with 4 Features\")\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Cross-Validation Accuracy: {np.mean(cv_scores * 100):.2f}% ± {np.std(cv_scores * 100):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V31gbxOKEpIN",
        "outputId": "ae06ea1d-7b85-48a7-f2f6-cac6d283e016"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Removing classes with fewer than 6 samples before SMOTE: [36, 21, 16, 41, 49, 28, 25, 33, 29, 47, 35, 23, 53, 5, 10, 0, 54, 26, 13, 44, 27, 24, 22, 42, 9, 43, 57, 51, 14, 40, 30, 48, 1, 37, 3, 18, 31, 15, 17]\n",
            "Stacking Classifier with 4 Features\n",
            "Test Accuracy: 19.27%\n",
            "Cross-Validation Accuracy: 76.47% ± 2.52%\n"
          ]
        }
      ]
    }
  ]
}